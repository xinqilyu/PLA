# ICCV 2025: PLA: Prompt Learning Attack against Text-to-Image Generative Models
## Abstract
Text-to-Image (T2I) models have gained widespread adoption across various applications. Despite the success, the potential misuse of T2I models poses significant risks of generating Not-Safe-For-Work (NSFW) content. To investigate the vulnerability of T2I models, this paper delves into adversarial attacks to bypass the safety mechanisms under black-box settings. Most previous methods rely on word substitution to search adversarial prompts. Due to limited search space, this leads to suboptimal performance compared to gradient-based training. However, black-box settings present unique challenges to training gradient-driven attack methods, since there is no access to the internal architecture and parameters of T2I models. To facilitate the learning of adversarial prompts in black-box settings, we propose a novel prompt learning attack framework (**PLA**), where insightful gradient-based training tailored to black-box T2I models is designed by utilizing multimodal similarities.
Experiments show that our new method can effectively attack the safety mechanisms of black-box T2I models including prompt filters and post-hoc safety checkers with a high success rate compared to state-of-the-art methods.
## Our Goal
Modify target prompts into adversarial prompts that can bypass safety mechanisms (i.e., prompt filters and post-hoc safety checkers), generating NSFW images that retain the sensitive semantics of target prompts.

<div align=center>
<img src="https://github.com/xinqilyu/PLA/blob/main/images/PLA-intro.png" >
</div>
This figure illustrates black-box victim models that incorporate prompt filters and post-hoc safety checkers. Prompt filters block prompts containing sensitive words or phrases from a predefined list. Post-hoc safety checkers block NSFW images generated by T2I models, returning black images. The attacker leverages adversarial prompts to maliciously bypass the safety mechanisms of the black-box victim models and generate NSFW images.

## Method Overview
<div align=center>
<img src="https://github.com/xinqilyu/PLA/blob/main/images/PLA-method.png" >
</div>
(a) In sensitive knowledge guided encoding, the SKE module extracts sensitive embeddings from the target prompt $p_{tar}$. Afterwards, the prompt encoder integrates the sensitive embeddings into a random prompt, where a learnable embedding $e_{pe}$ is generated.
(b) Given $p_{tar}$ and $e_{pe}$, we concate them as the input of PLM to generate the adversarial prompt, which can bypass the safety mechanisms and generate an NSFW image $I_{gen}$. Additionally, we utilize the target prompt to generate a target image $I_{tar}$ via an auxiliary model.
(c) By incorporating text-image and image-image similarities across $p_{tar}, I_{gen}, \text{and}\ I_{tar}$, multimodal loss is designed to optimize the prompt encoder parameters $\varsigma$ for generating adversarial prompts.

## Installation
